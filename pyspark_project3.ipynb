{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AYISHETU SEIDU\n",
    "\n",
    "BLOSSOM ACADEMY\n",
    "\n",
    "DATA ENGINEERING - SPRING 2020\n",
    "\n",
    "DATA TRANSFORMATION WITH PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "s3 = boto3.client('s3', region_name=\"eu-west-1\")\n",
    "\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import NGram,Tokenizer\n",
    "\n",
    "# create spark session if one doesn't exist already \n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download companies and job postings file from AWS S3 bucket\n",
    "s3.download_file(\"blossom-data-engs\", \"companies.csv\", 'companies.csv')\n",
    "s3.download_file(\"blossom-data-engs\", \"alldata.csv\", 'alldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ticker',\n",
       " 'company name',\n",
       " 'short name',\n",
       " 'industry',\n",
       " 'description',\n",
       " 'website',\n",
       " 'logo',\n",
       " 'ceo',\n",
       " 'exchange',\n",
       " 'market cap',\n",
       " 'sector',\n",
       " 'tag 1',\n",
       " 'tag 2',\n",
       " 'tag 3\\r']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the companies file as comp and inspect columns\n",
    "comp = spark.read.csv(\"companies.csv\", header=True, inferSchema=True, multiLine=True, escape='\"')\n",
    "comp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename inpappropriately named columns\n",
    "comp = comp.withColumnRenamed('company name','company_name')\n",
    "comp = comp.withColumnRenamed('short name','short_name')\n",
    "comp = comp.withColumnRenamed('market cap','market_cap')\n",
    "comp = comp.withColumnRenamed('tag 1','tag1')\n",
    "comp = comp.withColumnRenamed('tag 2','tag2')\n",
    "comp = comp.withColumnRenamed('tag 3\\r','tag3')\n",
    "\n",
    "#convert the compnay name to lower case to facilitate easy comparison with other datasets\n",
    "comp = comp.withColumn(\"company_name\",F.lower(F.col(\"company_name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['position', 'company', 'description', 'reviews', 'location\\r']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the job postings \"alldata.csv\" and inspect columns\n",
    "jobs = spark.read.csv(\"alldata.csv\", header=True, inferSchema=True, multiLine=True, escape='\"')\n",
    "jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['position', 'company', 'job_description', 'reviews', 'job_location']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename inpappropriately named columns and columns common in the two data sets to prevent ambiguity in query\n",
    "jobs = jobs.withColumnRenamed('description','job_description')\n",
    "jobs = jobs.withColumnRenamed('location\\r','job_location')\n",
    "\n",
    "jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract city from the location column, delimited by comma and name the resulting column'city'\n",
    "city = F.split(jobs['job_location'], ',')[0].alias('city')\n",
    "\n",
    "#add the resulting column from the split to the jobs dataframe\n",
    "jobs = jobs.withColumn('city', city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the company name and job description columns to lower case for easy analysis\n",
    "jobs = jobs.withColumn(\"company\",F.lower(F.col(\"company\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['position',\n",
       " 'company',\n",
       " 'job_description',\n",
       " 'reviews',\n",
       " 'job_location',\n",
       " 'city',\n",
       " 'ticker',\n",
       " 'company_name',\n",
       " 'short_name',\n",
       " 'industry',\n",
       " 'description',\n",
       " 'website',\n",
       " 'logo',\n",
       " 'ceo',\n",
       " 'exchange',\n",
       " 'market_cap',\n",
       " 'sector',\n",
       " 'tag1',\n",
       " 'tag2',\n",
       " 'tag3']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join jobs with companies using the company names and assign the resulting dataset to a new variable\n",
    "company_jobs = jobs.join(comp, jobs.company==comp.company_name)\n",
    "\n",
    "#inspect resulting dataset from the join\n",
    "company_jobs.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokens = Tokenizer(inputCol = 'job_description', outputCol = 'tokens')\n",
    "tokens = tokens.transform(employee_companies)\n",
    "tokens.select('tokens').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the job_description column to lower case\n",
    "company_jobs = company_jobs.withColumn(\"job_description\",F.lower(F.col(\"job_description\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to return n grams given dataset, column to convert and the number of n in the ngrams\n",
    "def create_ngrams(dataset, column, num_of_n):\n",
    "    from pyspark.ml.feature import NGram,Tokenizer\n",
    "    #tokenise the column values\n",
    "    tokens = Tokenizer(inputCol=column, outputCol='tokens')\n",
    "    tokens = tokens.transform(dataset)\n",
    "    #transform the tokens into ngrams and retrun the resulting output as a dataset\n",
    "    ng_data = NGram(n=num_of_n, inputCol='tokens', outputCol='ngrams')\n",
    "    ng_data = ng_data.transform(tokens)\n",
    "    return ng_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[position: string, company: string, job_description: string, reviews: int, job_location: string, city: string, ticker: string, company_name: string, short_name: string, industry: string, description: string, website: string, logo: string, ceo: string, exchange: string, market_cap: bigint, sector: string, tag1: string, tag2: string, tag3: string, tokens: array<string>, ngrams: array<string>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test ngram function\n",
    "create_ngrams(company_jobs, 'job_description',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a funtion that returns a 3 column data frame of ngrams, column_togroup_by and count of ngrams\n",
    "def create_2column_data(dataset, ngram_column,display_column, n):\n",
    "    #call the create_ngrams function\n",
    "    grams  = create_ngrams(dataset, ngram_column,n)\n",
    "    #explode the ngrams into new rows\n",
    "    data = grams.withColumn('ngrams',F.explode(grams.ngrams))\n",
    "    #count each ngram and group by the column specified in the function and return the resulting dataset\n",
    "    data = data.select('ngrams', display_column).groupby('ngrams',display_column).count()\n",
    "    return data\n",
    "#bigrams.select(['ngrams', 'city']).groupby('ngrams', 'city').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+\n",
      "|      ngrams|   city|count|\n",
      "+------------+-------+-----+\n",
      "|integration,| Austin|    1|\n",
      "|       siri,| Austin|    1|\n",
      "|   excellent| Austin|    1|\n",
      "|    relevant| Austin|    6|\n",
      "|           â€“| Austin|    1|\n",
      "|         key|Boulder|    1|\n",
      "|         her|Boulder|    1|\n",
      "|     tables,| Boston|    2|\n",
      "|      status| Boston|    2|\n",
      "|   establish|Chicago|    1|\n",
      "|       teams|Chicago|   12|\n",
      "|         (or|Chicago|    3|\n",
      "| skillsshare|Chicago|    1|\n",
      "|      401(k)|Chicago|    1|\n",
      "|      master|Chicago|    1|\n",
      "|   required:|Chicago|    2|\n",
      "|     systems|Chicago|    2|\n",
      "|     cutting|Chicago|    2|\n",
      "|  strategies|Chicago|    3|\n",
      "|        true|Chicago|    1|\n",
      "+------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use the create_2colucreate_2column_data to create a unigram and group ngrams by city\n",
    "city = create_2column_data(company_jobs, 'job_description','city', 1)\n",
    "city.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|              ngrams|            industry|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|           barkthins|Consumer Packaged...|    1|\n",
      "|             creates|Consumer Packaged...|    1|\n",
      "|        formulation,|Consumer Packaged...|    1|\n",
      "|            payments|   Health Care Plans|    2|\n",
      "|           retention|   Health Care Plans|    1|\n",
      "|       interpersonal|   Health Care Plans|    1|\n",
      "|           business.|Retail - Apparel ...|    1|\n",
      "|          employment|Retail - Apparel ...|    3|\n",
      "|        development,|   Computer Hardware|    1|\n",
      "|                  at|   Computer Hardware|    1|\n",
      "|        agriculture,|   Computer Hardware|    1|\n",
      "|            required|Medical Diagnosti...|    2|\n",
      "|                some|Engineering & Con...|    4|\n",
      "|              signed|Engineering & Con...|    6|\n",
      "|implementationstrong|           Insurance|    1|\n",
      "|                 -in|           Insurance|    1|\n",
      "|              highly|     Credit Services|    3|\n",
      "|       additionally,|     Credit Services|    1|\n",
      "|          compliance|     Credit Services|    2|\n",
      "|               peers|     Credit Services|    2|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use the create_2colucreate_2column_data to create a unigram and group ngrams by industry\n",
    "industry1 = create_2column_data(company_jobs, 'job_description','industry', 1)\n",
    "industry1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+\n",
      "|              ngrams|   city|count|\n",
      "+--------------------+-------+-----+\n",
      "|  related challenges| Austin|    1|\n",
      "|     college degree,| Austin|    1|\n",
      "|   opportunities for| Austin|    1|\n",
      "|         skills used| Austin|    1|\n",
      "|      color, gender,| Austin|    1|\n",
      "|        life. today,| Austin|    1|\n",
      "|          study data| Austin|    1|\n",
      "| economics functions| Austin|    1|\n",
      "|        and content.| Austin|    1|\n",
      "|    proven technical| Austin|    1|\n",
      "|      versatile work| Austin|    1|\n",
      "|         sex, sexual| Austin|    1|\n",
      "|            taking a|Boulder|    1|\n",
      "|professional comm...|Boulder|    1|\n",
      "|        when needed.|Boulder|    1|\n",
      "|      and logistics.|Boulder|    1|\n",
      "|     and affirmative|Boulder|    1|\n",
      "|   protected factor.|Boulder|    1|\n",
      "|         services to| Boston|    1|\n",
      "|      development of| Boston|    5|\n",
      "+--------------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use the create_2colucreate_2column_data to create a bigram and group by city\n",
    "city2 = create_2column_data(company_jobs, 'job_description','city', 2)\n",
    "city2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|              ngrams|            industry|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|   between disparate| Aerospace & Defense|    1|\n",
      "|            in clear| Aerospace & Defense|    1|\n",
      "|analysis methodology| Aerospace & Defense|    1|\n",
      "|      manner through| Aerospace & Defense|    1|\n",
      "|          and around|Consumer Packaged...|    1|\n",
      "|     masterâ€™s degree|     Medical Devices|    2|\n",
      "|    understanding of|   Health Care Plans|    1|\n",
      "|         will ensure|   Health Care Plans|    1|\n",
      "|            within a|   Health Care Plans|    2|\n",
      "|    complex analyses|   Health Care Plans|    1|\n",
      "|machines, supervi...|   Health Care Plans|    1|\n",
      "|theoretical knowl...|   Health Care Plans|    1|\n",
      "|           this role|Retail - Apparel ...|    3|\n",
      "|deployment infras...|Retail - Apparel ...|    1|\n",
      "|       aggregate and|Retail - Apparel ...|    1|\n",
      "|   computing systems|Retail - Apparel ...|    1|\n",
      "|wsdl/soap interfa...|Retail - Apparel ...|    1|\n",
      "|             area of|Retail - Apparel ...|    1|\n",
      "|         in boulder,|   Computer Hardware|    1|\n",
      "|          a customer|   Computer Hardware|    1|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use the create_2colucreate_2column_data to create a bigram and group by industry\n",
    "industry2 = create_2column_data(company_jobs, 'job_description','industry', 2)\n",
    "industry2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
